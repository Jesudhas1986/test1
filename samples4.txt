How to Fix It
✅ Option 1: Add a Toleration to the Pod or DeploymentConfig
This will allow the pod to be scheduled on the tainted master node.

For a DeploymentConfig:
bash
Copy
Edit
oc patch dc <your-dc-name> --type='merge' -p '{
  "spec": {
    "template": {
      "spec": {
        "tolerations": [
          {
            "key": "node-role.kubernetes.io/master",
            "operator": "Exists",
            "effect": "NoSchedule"
          }
        ]
      }
    }
  }
}'
Or manually edit:
bash
Copy
Edit
oc edit dc <your-dc-name>
Add under spec.template.spec:

yaml
Copy
Edit
tolerations:
- key: "node-role.kubernetes.io/master"
  operator: "Exists"
  effect: "NoSchedule"
✅ Option 2: Avoid Master Nodes (Recommended in Production)
Instead of tolerating the taint, make sure your pod schedules only on worker nodes.

You can do this by not using any tolerations, or using a nodeSelector like this (if your workers are labeled):

yaml
Copy
Edit
nodeSelector:
  node-role.kubernetes.io/worker: ""
⚠️ Warning:
Running regular workloads on master/control-plane nodes is not recommended in production. It can interfere with cluster stability.

✅ Summary
Solution	Description
Add toleration for NoSchedule	Allows pod to run on master (not recommended)
Use nodeSelector for workers	Ensures pod avoids master and targets worker nodes
Do not modify tolerations	Scheduler will naturally avoid tainted master nodes